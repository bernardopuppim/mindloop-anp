{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANP PDF to Knowledge Graph & Policy Tree\n",
    "\n",
    "**Objetivo**: Construir um pipeline completo de extra√ß√£o de texto de PDFs normativos da ANP, gera√ß√£o de Knowledge Graph orientado a decis√£o, proje√ß√£o em Policy Graph (DAG decis√≥rio) e compila√ß√£o final em √°rvore JSON compat√≠vel com classificador LATS.\n",
    "\n",
    "**Outputs**:\n",
    "- `artifacts/anp_text_corpus.jsonl` - Textos limpos por PDF\n",
    "- `artifacts/anp_kg.graphml` - Knowledge Graph completo\n",
    "- `artifacts/anp_policy.graphml` - Policy Graph (DAG decis√≥rio)\n",
    "- `artifacts/anp_tree.json` - √Årvore de decis√£o final\n",
    "\n",
    "**Vers√£o**: 1.0  \n",
    "**Data**: 2025-12-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0] Setup e Imports\n",
    "\n",
    "### Depend√™ncias necess√°rias:\n",
    "\n",
    "```bash\n",
    "pip install pymupdf pdfplumber pytesseract pillow langchain langchain-experimental langchain-openai networkx pydantic python-dotenv tqdm matplotlib\n",
    "```\n",
    "\n",
    "**Nota**: Para OCR, √© necess√°rio ter o Tesseract instalado no sistema:\n",
    "- Ubuntu/Debian: `sudo apt-get install tesseract-ocr tesseract-ocr-por`\n",
    "- macOS: `brew install tesseract tesseract-lang`\n",
    "- Windows: Download do instalador em https://github.com/UB-Mannheim/tesseract/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è pytesseract n√£o dispon√≠vel. OCR ser√° desabilitado.\n",
      "‚úÖ Imports carregados com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Imports padr√£o\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import hashlib\n",
    "\n",
    "# Processamento de PDF e OCR\n",
    "import fitz  # PyMuPDF\n",
    "try:\n",
    "    import pytesseract\n",
    "    from PIL import Image\n",
    "    OCR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OCR_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è pytesseract n√£o dispon√≠vel. OCR ser√° desabilitado.\")\n",
    "\n",
    "# Grafo e an√°lise\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# LangChain e LLM\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Configura√ß√£o\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports carregados com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Diret√≥rio de PDFs encontrado: ../padroes_anp\n",
      "\n",
      "üìÇ Configura√ß√£o:\n",
      "   PDFs: /home/puppyn/projects/ANP_classifier/notebooks/../padroes_anp\n",
      "   Artefatos: /home/puppyn/projects/ANP_classifier/notebooks/../artifacts\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o de diret√≥rios\n",
    "PDF_DIR = Path(\"../padroes_anp\")\n",
    "ARTIFACTS_DIR = Path(\"../artifacts\")\n",
    "\n",
    "# Criar diret√≥rio de artefatos se n√£o existir\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verificar se diret√≥rio de PDFs existe\n",
    "if not PDF_DIR.exists():\n",
    "    print(f\"‚ö†Ô∏è Diret√≥rio {PDF_DIR} n√£o encontrado. Criando...\")\n",
    "    PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    print(f\"‚úÖ Diret√≥rio de PDFs encontrado: {PDF_DIR}\")\n",
    "\n",
    "# Arquivos de sa√≠da\n",
    "CORPUS_FILE = ARTIFACTS_DIR / \"anp_text_corpus.jsonl\"\n",
    "KG_GRAPHML = ARTIFACTS_DIR / \"anp_kg.graphml\"\n",
    "KG_JSON = ARTIFACTS_DIR / \"anp_kg.json\"\n",
    "POLICY_GRAPHML = ARTIFACTS_DIR / \"anp_policy.graphml\"\n",
    "POLICY_JSON = ARTIFACTS_DIR / \"anp_policy.json\"\n",
    "TREE_JSON = ARTIFACTS_DIR / \"anp_tree.json\"\n",
    "\n",
    "print(f\"\\nüìÇ Configura√ß√£o:\")\n",
    "print(f\"   PDFs: {PDF_DIR.absolute()}\")\n",
    "print(f\"   Artefatos: {ARTIFACTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Descobrir PDFs\n",
    "\n",
    "Listar todos os PDFs no diret√≥rio e exibir informa√ß√µes b√°sicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö PDFs Encontrados: 2\n",
      "\n",
      "Filename                                           P√°ginas    Tamanho (MB)    Doc ID\n",
      "------------------------------------------------------------------------------------------\n",
      "manual-comunicacao-incidentes-ANP.pdf              99         3.07            2c8962cfe16d\n",
      "resolucao-anp-n-882-2022.pdf                       5          1.77            8b0048e9291b\n"
     ]
    }
   ],
   "source": [
    "def descobrir_pdfs(pdf_dir: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Descobre todos os PDFs no diret√≥rio especificado.\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Diret√≥rio contendo os PDFs\n",
    "        \n",
    "    Returns:\n",
    "        Lista de dicion√°rios com informa√ß√µes sobre cada PDF\n",
    "    \"\"\"\n",
    "    pdfs = []\n",
    "    \n",
    "    for pdf_path in sorted(pdf_dir.glob(\"*.pdf\")):\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            pdfs.append({\n",
    "                \"path\": pdf_path,\n",
    "                \"filename\": pdf_path.name,\n",
    "                \"size_mb\": pdf_path.stat().st_size / (1024 * 1024),\n",
    "                \"num_pages\": len(doc),\n",
    "                \"doc_id\": hashlib.md5(pdf_path.name.encode()).hexdigest()[:12]\n",
    "            })\n",
    "            doc.close()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao abrir {pdf_path.name}: {e}\")\n",
    "    \n",
    "    return pdfs\n",
    "\n",
    "# Descobrir PDFs\n",
    "pdfs_info = descobrir_pdfs(PDF_DIR)\n",
    "\n",
    "print(f\"\\nüìö PDFs Encontrados: {len(pdfs_info)}\\n\")\n",
    "print(f\"{'Filename':<50} {'P√°ginas':<10} {'Tamanho (MB)':<15} {'Doc ID'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for info in pdfs_info:\n",
    "    print(f\"{info['filename']:<50} {info['num_pages']:<10} {info['size_mb']:<15.2f} {info['doc_id']}\")\n",
    "\n",
    "if len(pdfs_info) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Nenhum PDF encontrado. Coloque os arquivos PDF em:\", PDF_DIR.absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Extra√ß√£o de Texto\n",
    "\n",
    "Extrai texto de cada PDF usando PyMuPDF, com fallback para OCR quando necess√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extraindo texto dos PDFs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extra√≠dos 2 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extrair_texto_pagina(page, min_chars: int = 30) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extrai texto de uma p√°gina PDF, com fallback para OCR.\n",
    "    \n",
    "    Args:\n",
    "        page: P√°gina do PyMuPDF\n",
    "        min_chars: M√≠nimo de caracteres para considerar extra√ß√£o bem-sucedida\n",
    "        \n",
    "    Returns:\n",
    "        Tupla (texto, m√©todo) onde m√©todo √© \"text\" ou \"ocr\"\n",
    "    \"\"\"\n",
    "    # Tentar extra√ß√£o direta de texto\n",
    "    text = page.get_text(\"text\")\n",
    "    \n",
    "    # Se texto for muito curto, tentar OCR\n",
    "    if len(text.strip()) < min_chars and OCR_AVAILABLE:\n",
    "        try:\n",
    "            # Renderizar p√°gina como imagem\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom para melhor OCR\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            \n",
    "            # OCR\n",
    "            text_ocr = pytesseract.image_to_string(img, lang='por')\n",
    "            \n",
    "            if len(text_ocr.strip()) > len(text.strip()):\n",
    "                return text_ocr, \"ocr\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro no OCR: {e}\")\n",
    "    \n",
    "    return text, \"text\"\n",
    "\n",
    "\n",
    "def extrair_pdf_completo(pdf_path: Path, doc_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extrai texto completo de um PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Caminho do PDF\n",
    "        doc_id: ID √∫nico do documento\n",
    "        \n",
    "    Returns:\n",
    "        Dicion√°rio com metadados e texto extra√≠do\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_data = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text, method = extrair_texto_pagina(page)\n",
    "        \n",
    "        pages_data.append({\n",
    "            \"page\": page_num + 1,\n",
    "            \"method\": method,\n",
    "            \"text\": text\n",
    "        })\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"filename\": pdf_path.name,\n",
    "        \"pages\": pages_data,\n",
    "        \"extracted_at\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# Extrair texto de todos os PDFs\n",
    "print(\"\\nüìÑ Extraindo texto dos PDFs...\\n\")\n",
    "\n",
    "raw_extractions = []\n",
    "for pdf_info in tqdm(pdfs_info, desc=\"Processando PDFs\"):\n",
    "    extraction = extrair_pdf_completo(pdf_info[\"path\"], pdf_info[\"doc_id\"])\n",
    "    raw_extractions.append(extraction)\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    ocr_pages = sum(1 for p in extraction[\"pages\"] if p[\"method\"] == \"ocr\")\n",
    "    if ocr_pages > 0:\n",
    "        print(f\"  {pdf_info['filename']}: {ocr_pages}/{len(extraction['pages'])} p√°ginas via OCR\")\n",
    "\n",
    "print(f\"\\n‚úÖ Extra√≠dos {len(raw_extractions)} documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Limpeza e Normaliza√ß√£o\n",
    "\n",
    "Normaliza espa√ßos, remove headers/footers repetitivos, corrige hifeniza√ß√£o e problemas de encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Limpando e normalizando textos...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limpando documentos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 34.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Corpus salvo em: ../artifacts/anp_text_corpus.jsonl\n",
      "\n",
      "üìä Estat√≠sticas do Corpus:\n",
      "   Total de documentos: 2\n",
      "   Total de caracteres: 169,291\n",
      "   Total de palavras: 24,923\n",
      "   M√©dia palavras/doc: 12462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza espa√ßos em branco, tabs e quebras de linha.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a normalizar\n",
    "        \n",
    "    Returns:\n",
    "        Texto normalizado\n",
    "    \"\"\"\n",
    "    # Substituir m√∫ltiplos espa√ßos por um √∫nico\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Substituir m√∫ltiplas quebras de linha por no m√°ximo duas\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    # Remover espa√ßos no in√≠cio e fim de linhas\n",
    "    text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_page_headers_footers(pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Remove headers e footers repetitivos detectando linhas que aparecem em muitas p√°ginas.\n",
    "    \n",
    "    Args:\n",
    "        pages: Lista de p√°ginas com texto\n",
    "        \n",
    "    Returns:\n",
    "        Lista de p√°ginas com headers/footers removidos\n",
    "    \"\"\"\n",
    "    if len(pages) < 3:\n",
    "        return pages\n",
    "    \n",
    "    # Coletar primeiras e √∫ltimas 3 linhas de cada p√°gina\n",
    "    first_lines = defaultdict(int)\n",
    "    last_lines = defaultdict(int)\n",
    "    \n",
    "    for page in pages:\n",
    "        lines = page[\"text\"].split('\\n')\n",
    "        if len(lines) > 6:\n",
    "            for line in lines[:3]:\n",
    "                line_clean = line.strip()\n",
    "                if len(line_clean) > 5:  # Ignorar linhas muito curtas\n",
    "                    first_lines[line_clean] += 1\n",
    "            for line in lines[-3:]:\n",
    "                line_clean = line.strip()\n",
    "                if len(line_clean) > 5:\n",
    "                    last_lines[line_clean] += 1\n",
    "    \n",
    "    # Detectar linhas que aparecem em > 50% das p√°ginas\n",
    "    threshold = len(pages) * 0.5\n",
    "    headers = {line for line, count in first_lines.items() if count > threshold}\n",
    "    footers = {line for line, count in last_lines.items() if count > threshold}\n",
    "    \n",
    "    # Remover headers/footers\n",
    "    cleaned_pages = []\n",
    "    for page in pages:\n",
    "        lines = page[\"text\"].split('\\n')\n",
    "        cleaned_lines = [line for line in lines if line.strip() not in headers and line.strip() not in footers]\n",
    "        \n",
    "        cleaned_pages.append({\n",
    "            **page,\n",
    "            \"text\": '\\n'.join(cleaned_lines)\n",
    "        })\n",
    "    \n",
    "    return cleaned_pages\n",
    "\n",
    "\n",
    "def dehyphenate(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Junta palavras quebradas por h√≠fen no final de linha.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a processar\n",
    "        \n",
    "    Returns:\n",
    "        Texto com hifeniza√ß√£o corrigida\n",
    "    \"\"\"\n",
    "    # Padr√£o: h√≠fen no final de linha seguido por quebra e palavra\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def fix_encoding_artifacts(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Corrige artefatos comuns de encoding.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a corrigir\n",
    "        \n",
    "    Returns:\n",
    "        Texto com encoding corrigido\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        '√É¬ß': '√ß',\n",
    "        '√É¬£': '√£',\n",
    "        '√É¬©': '√©',\n",
    "        '√É¬°': '√°',\n",
    "        '√É¬≥': '√≥',\n",
    "        '√É¬™': '√™',\n",
    "        '√É¬¥': '√¥',\n",
    "        '√É': '√≠',\n",
    "        '√É¬∫': '√∫',\n",
    "    }\n",
    "    \n",
    "    for wrong, correct in replacements.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_document(extraction: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aplica pipeline completo de limpeza em um documento.\n",
    "    \n",
    "    Args:\n",
    "        extraction: Dicion√°rio com extra√ß√£o bruta\n",
    "        \n",
    "    Returns:\n",
    "        Dicion√°rio com texto limpo\n",
    "    \"\"\"\n",
    "    # Remover headers/footers\n",
    "    pages_cleaned = remove_page_headers_footers(extraction[\"pages\"])\n",
    "    \n",
    "    # Concatenar todas as p√°ginas\n",
    "    full_text = \"\\n\\n\".join(page[\"text\"] for page in pages_cleaned)\n",
    "    \n",
    "    # Aplicar limpezas\n",
    "    full_text = fix_encoding_artifacts(full_text)\n",
    "    full_text = dehyphenate(full_text)\n",
    "    full_text = normalize_whitespace(full_text)\n",
    "    \n",
    "    return {\n",
    "        \"doc_id\": extraction[\"doc_id\"],\n",
    "        \"filename\": extraction[\"filename\"],\n",
    "        \"text_clean\": full_text,\n",
    "        \"num_chars\": len(full_text),\n",
    "        \"num_words\": len(full_text.split())\n",
    "    }\n",
    "\n",
    "\n",
    "# Limpar todos os documentos\n",
    "print(\"\\nüßπ Limpando e normalizando textos...\\n\")\n",
    "\n",
    "clean_docs = []\n",
    "for extraction in tqdm(raw_extractions, desc=\"Limpando documentos\"):\n",
    "    clean_doc = clean_document(extraction)\n",
    "    clean_docs.append(clean_doc)\n",
    "\n",
    "# Salvar corpus limpo\n",
    "with open(CORPUS_FILE, 'w', encoding='utf-8') as f:\n",
    "    for doc in clean_docs:\n",
    "        f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\\n‚úÖ Corpus salvo em: {CORPUS_FILE}\")\n",
    "print(f\"\\nüìä Estat√≠sticas do Corpus:\")\n",
    "print(f\"   Total de documentos: {len(clean_docs)}\")\n",
    "print(f\"   Total de caracteres: {sum(doc['num_chars'] for doc in clean_docs):,}\")\n",
    "print(f\"   Total de palavras: {sum(doc['num_words'] for doc in clean_docs):,}\")\n",
    "print(f\"   M√©dia palavras/doc: {sum(doc['num_words'] for doc in clean_docs) / len(clean_docs):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Chunking\n",
    "\n",
    "Divide documentos em chunks baseados em se√ß√µes detectadas ou por tamanho fixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÇÔ∏è Gerando chunks...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documentos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 106.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  manual-comunicacao-incidentes-ANP.pdf: 43 chunks\n",
      "  resolucao-anp-n-882-2022.pdf: 0 chunks\n",
      "\n",
      "‚úÖ Total de chunks: 43\n",
      "   Tamanho m√©dio: 3934 caracteres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def detectar_secoes(text: str) -> List[Tuple[int, str, str]]:\n",
    "    \"\"\"\n",
    "    Detecta se√ß√µes no texto usando heur√≠sticas.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a processar\n",
    "        \n",
    "    Returns:\n",
    "        Lista de tuplas (posi√ß√£o, tipo_secao, t√≠tulo)\n",
    "    \"\"\"\n",
    "    secoes = []\n",
    "    \n",
    "    # Padr√µes de t√≠tulos\n",
    "    patterns = [\n",
    "        (r'^(CAP√çTULO|SE√á√ÉO|ANEXO|ARTIGO)\\s+[IVX\\d]+', 'capitulo'),\n",
    "        (r'^Art\\.\\s*\\d+', 'artigo'),\n",
    "        (r'^\\d+\\.\\s+[A-Z√Ä√Å√É√Ç√â√ä√ç√ì√î√ï√ö][A-Z√Ä√Å√É√Ç√â√ä√ç√ì√î√ï√ö\\s]{5,}$', 'titulo_caps'),\n",
    "        (r'^[A-Z√Ä√Å√É√Ç√â√ä√ç√ì√î√ï√ö][A-Z√Ä√Å√É√Ç√â√ä√ç√ì√î√ï√ö\\s]{10,}$', 'secao_caps'),\n",
    "    ]\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    pos = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        for pattern, tipo in patterns:\n",
    "            if re.match(pattern, line_stripped, re.MULTILINE):\n",
    "                secoes.append((pos, tipo, line_stripped))\n",
    "                break\n",
    "        pos += len(line) + 1  # +1 para o \\n\n",
    "    \n",
    "    return secoes\n",
    "\n",
    "\n",
    "def chunk_por_secoes(doc: Dict[str, Any], max_chunk_chars: int = 6000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Divide documento em chunks baseados em se√ß√µes detectadas.\n",
    "    \n",
    "    Args:\n",
    "        doc: Documento limpo\n",
    "        max_chunk_chars: Tamanho m√°ximo de chunk (em caracteres)\n",
    "        \n",
    "    Returns:\n",
    "        Lista de chunks\n",
    "    \"\"\"\n",
    "    text = doc[\"text_clean\"]\n",
    "    secoes = detectar_secoes(text)\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    if len(secoes) == 0:\n",
    "        # Fallback: chunk por tamanho fixo\n",
    "        for i in range(0, len(text), max_chunk_chars):\n",
    "            chunk_text = text[i:i + max_chunk_chars]\n",
    "            chunks.append({\n",
    "                \"chunk_id\": f\"{doc['doc_id']}_chunk_{len(chunks)}\",\n",
    "                \"doc_id\": doc[\"doc_id\"],\n",
    "                \"section_hint\": \"chunk_fixo\",\n",
    "                \"text\": chunk_text\n",
    "            })\n",
    "    else:\n",
    "        # Chunk por se√ß√µes\n",
    "        for i, (pos, tipo, titulo) in enumerate(secoes):\n",
    "            # Encontrar fim da se√ß√£o (in√≠cio da pr√≥xima ou fim do texto)\n",
    "            if i < len(secoes) - 1:\n",
    "                end_pos = secoes[i + 1][0]\n",
    "            else:\n",
    "                end_pos = len(text)\n",
    "            \n",
    "            chunk_text = text[pos:end_pos].strip()\n",
    "            \n",
    "            # Se chunk muito grande, dividir\n",
    "            if len(chunk_text) > max_chunk_chars:\n",
    "                for j in range(0, len(chunk_text), max_chunk_chars):\n",
    "                    sub_chunk = chunk_text[j:j + max_chunk_chars]\n",
    "                    chunks.append({\n",
    "                        \"chunk_id\": f\"{doc['doc_id']}_sec_{i}_part_{j // max_chunk_chars}\",\n",
    "                        \"doc_id\": doc[\"doc_id\"],\n",
    "                        \"section_hint\": f\"{tipo}:{titulo[:50]}\",\n",
    "                        \"text\": sub_chunk\n",
    "                    })\n",
    "            else:\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": f\"{doc['doc_id']}_sec_{i}\",\n",
    "                    \"doc_id\": doc[\"doc_id\"],\n",
    "                    \"section_hint\": f\"{tipo}:{titulo[:50]}\",\n",
    "                    \"text\": chunk_text\n",
    "                })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Gerar chunks de todos os documentos\n",
    "print(\"\\n‚úÇÔ∏è Gerando chunks...\\n\")\n",
    "\n",
    "all_chunks = []\n",
    "for doc in tqdm(clean_docs, desc=\"Chunking documentos\"):\n",
    "    doc_chunks = chunk_por_secoes(doc)\n",
    "    all_chunks.extend(doc_chunks)\n",
    "    print(f\"  {doc['filename']}: {len(doc_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total de chunks: {len(all_chunks)}\")\n",
    "print(f\"   Tamanho m√©dio: {sum(len(c['text']) for c in all_chunks) / len(all_chunks):.0f} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] LLM Config (Azure OpenAI)\n",
    "\n",
    "Configurar conex√£o com Azure OpenAI para gera√ß√£o do Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vari√°veis de ambiente carregadas\n",
      "\n",
      "ü§ñ LLM configurado:\n",
      "   Provider: OpenAI\n",
      "   Model: gpt-4o-mini\n",
      "   Temperature: 0 (determin√≠stico)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Carregar vari√°veis de ambiente\n",
    "# --------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# Verificar configura√ß√£o\n",
    "required_env_vars = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"‚ùå Vari√°veis de ambiente faltando:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   - {var}\")\n",
    "    print(\"\\n‚ö†Ô∏è Configure o arquivo .env antes de continuar.\")\n",
    "else:\n",
    "    print(\"‚úÖ Vari√°veis de ambiente carregadas\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Instanciar LLM (OpenAI direto)\n",
    "# --------------------------------------------------\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # pode trocar para \"gpt-4o\" se quiser\n",
    "    temperature=0,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "print(\"\\nü§ñ LLM configurado:\")\n",
    "print(\"   Provider: OpenAI\")\n",
    "print(\"   Model: gpt-4o-mini\")\n",
    "print(\"   Temperature: 0 (determin√≠stico)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] Knowledge Graph com LLMGraphTransformer\n",
    "\n",
    "Gera Knowledge Graph orientado a decis√£o usando LLM para extrair entidades e rela√ß√µes normativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLMGraphTransformer configurado\n",
      "\n",
      "üìã Schema Decisional:\n",
      "   Tipos de n√≥s: 8\n",
      "   Tipos de rela√ß√µes: 8\n"
     ]
    }
   ],
   "source": [
    "# Schema decisional para o KG\n",
    "ALLOWED_NODES = [\n",
    "    \"IncidentType\",      # Tipo de incidente (les√£o, meio ambiente, etc)\n",
    "    \"Criterion\",         # Crit√©rio decis√≥rio (pergunta)\n",
    "    \"Threshold\",         # Limiar num√©rico (volume, dias, etc)\n",
    "    \"Classification\",    # Classifica√ß√£o final (Classe 1, 2, etc)\n",
    "    \"Obligation\",        # Obriga√ß√£o normativa\n",
    "    \"Exception\",         # Exce√ß√£o √† regra\n",
    "    \"Actor\",            # Ator envolvido (ANP, operador, etc)\n",
    "    \"Evidence\"          # Evid√™ncia necess√°ria\n",
    "]\n",
    "\n",
    "ALLOWED_RELATIONSHIPS = [\n",
    "    \"DEPENDS_ON\",        # Crit√©rio depende de outro\n",
    "    \"CLASSIFIED_AS\",     # Leva √† classifica√ß√£o\n",
    "    \"IMPLIES\",           # Implica consequ√™ncia\n",
    "    \"REQUIRES\",          # Requer evid√™ncia/a√ß√£o\n",
    "    \"HAS_THRESHOLD\",     # Possui limiar\n",
    "    \"HAS_EXCEPTION\",     # Possui exce√ß√£o\n",
    "    \"APPLIES_TO\",        # Aplica-se a\n",
    "    \"EVIDENCED_BY\"       # Evidenciado por\n",
    "]\n",
    "\n",
    "# Prompt guia para extra√ß√£o decisional\n",
    "DECISIONAL_GUIDE = \"\"\"Extraia apenas conceitos necess√°rios para CLASSIFICAR INCIDENTES segundo a norma ANP.\n",
    "\n",
    "FOQUE EM:\n",
    "- Crit√©rios pergunt√°veis (sim/n√£o, qual tipo, qual faixa)\n",
    "- Thresholds num√©ricos (volume > X, dias >= Y)\n",
    "- Exce√ß√µes e condi√ß√µes especiais\n",
    "- Mapeamento direto para classifica√ß√µes (Classe 1, 2, 3, etc)\n",
    "\n",
    "IGNORE:\n",
    "- Narrativa hist√≥rica\n",
    "- Exemplos irrelevantes para decis√£o\n",
    "- Contexto administrativo geral\n",
    "- Defini√ß√µes que n√£o afetam classifica√ß√£o\n",
    "\n",
    "IMPORTANTE: Mantenha foco em construir um grafo DECISIONAL, n√£o enciclop√©dico.\n",
    "\"\"\"\n",
    "\n",
    "# Configurar transformer\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=ALLOWED_NODES,\n",
    "    allowed_relationships=ALLOWED_RELATIONSHIPS,\n",
    "    node_properties=True,\n",
    "    relationship_properties=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLMGraphTransformer configurado\")\n",
    "print(f\"\\nüìã Schema Decisional:\")\n",
    "print(f\"   Tipos de n√≥s: {len(ALLOWED_NODES)}\")\n",
    "print(f\"   Tipos de rela√ß√µes: {len(ALLOWED_RELATIONSHIPS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è MODO DE TESTE: Processando apenas 10 chunks\n",
      "   Altere TEST_MODE = False para processar corpus completo\n",
      "\n",
      "\n",
      "üß† Processando 10 chunks com LLM...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando KG: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:35<00:00,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Knowledge Graph gerado:\n",
      "   N√≥s: 39\n",
      "   Arestas: 45\n",
      "\n",
      "üìä Distribui√ß√£o de N√≥s:\n",
      "   Criterion: 14\n",
      "   Classification: 11\n",
      "   Threshold: 5\n",
      "   Incidenttype: 5\n",
      "   Exception: 3\n",
      "   Actor: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def processar_chunks_para_kg(chunks: List[Dict[str, Any]], \n",
    "                             transformer: LLMGraphTransformer,\n",
    "                             max_chunks: Optional[int] = None) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Processa chunks e gera Knowledge Graph unificado.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Lista de chunks de texto\n",
    "        transformer: LLMGraphTransformer configurado\n",
    "        max_chunks: Limite de chunks a processar (para testes)\n",
    "        \n",
    "    Returns:\n",
    "        NetworkX DiGraph com KG unificado\n",
    "    \"\"\"\n",
    "    kg = nx.DiGraph()\n",
    "    \n",
    "    # Limitar chunks se especificado\n",
    "    chunks_to_process = chunks[:max_chunks] if max_chunks else chunks\n",
    "    \n",
    "    print(f\"\\nüß† Processando {len(chunks_to_process)} chunks com LLM...\\n\")\n",
    "    \n",
    "    for chunk in tqdm(chunks_to_process, desc=\"Gerando KG\"):\n",
    "        try:\n",
    "            # Preparar documento com guia decisional\n",
    "            doc_text = f\"{DECISIONAL_GUIDE}\\n\\n{chunk['text']}\"\n",
    "            doc = Document(page_content=doc_text, metadata={\"chunk_id\": chunk[\"chunk_id\"]})\n",
    "            \n",
    "            # Extrair grafo do chunk\n",
    "            graph_docs = transformer.convert_to_graph_documents([doc])\n",
    "            \n",
    "            # Adicionar ao KG unificado\n",
    "            for graph_doc in graph_docs:\n",
    "                # Adicionar n√≥s\n",
    "                for node in graph_doc.nodes:\n",
    "                    node_id = f\"{node.type}:{node.id}\"\n",
    "                    if node_id not in kg:\n",
    "                        kg.add_node(node_id, \n",
    "                                   type=node.type, \n",
    "                                   id=node.id,\n",
    "                                   properties=node.properties if hasattr(node, 'properties') else {})\n",
    "                \n",
    "                # Adicionar arestas\n",
    "                for rel in graph_doc.relationships:\n",
    "                    source_id = f\"{rel.source.type}:{rel.source.id}\"\n",
    "                    target_id = f\"{rel.target.type}:{rel.target.id}\"\n",
    "                    \n",
    "                    if not kg.has_edge(source_id, target_id):\n",
    "                        kg.add_edge(source_id, target_id,\n",
    "                                   type=rel.type,\n",
    "                                   properties=rel.properties if hasattr(rel, 'properties') else {})\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao processar chunk {chunk['chunk_id']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return kg\n",
    "\n",
    "\n",
    "# Gerar KG (NOTA: Processar todos os chunks pode levar muito tempo e custar muito)\n",
    "# Para teste inicial, processar apenas os primeiros N chunks\n",
    "TEST_MODE = True  # Altere para False para processar tudo\n",
    "MAX_CHUNKS_TEST = 10\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f\"\\n‚ö†Ô∏è MODO DE TESTE: Processando apenas {MAX_CHUNKS_TEST} chunks\")\n",
    "    print(\"   Altere TEST_MODE = False para processar corpus completo\\n\")\n",
    "    kg = processar_chunks_para_kg(all_chunks, graph_transformer, max_chunks=MAX_CHUNKS_TEST)\n",
    "else:\n",
    "    kg = processar_chunks_para_kg(all_chunks, graph_transformer)\n",
    "\n",
    "print(f\"\\n‚úÖ Knowledge Graph gerado:\")\n",
    "print(f\"   N√≥s: {kg.number_of_nodes()}\")\n",
    "print(f\"   Arestas: {kg.number_of_edges()}\")\n",
    "\n",
    "# Estat√≠sticas por tipo\n",
    "node_types = Counter(kg.nodes[n]['type'] for n in kg.nodes())\n",
    "print(f\"\\nüìä Distribui√ß√£o de N√≥s:\")\n",
    "for node_type, count in node_types.most_common():\n",
    "    print(f\"   {node_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Salvando Knowledge Graph...\n",
      "   ‚úÖ JSON: ../artifacts/anp_kg.json\n"
     ]
    }
   ],
   "source": [
    "# Exportar KG\n",
    "print(\"\\nüíæ Salvando Knowledge Graph...\")\n",
    "\n",
    "# JSON (node-link format)\n",
    "kg_json = nx.node_link_data(kg)\n",
    "with open(KG_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(kg_json, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ‚úÖ JSON: {KG_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7] Policy Graph (Proje√ß√£o do KG)\n",
    "\n",
    "Projeta o KG em um Policy Graph decis√≥rio, focando em crit√©rios, thresholds e classifica√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Gerando Policy Graph...\n",
      "\n",
      "\n",
      "üìä Policy Graph:\n",
      "   N√≥s: 38\n",
      "   Arestas: 14\n",
      "\n",
      "üìä Distribui√ß√£o de N√≥s (Policy):\n",
      "   Criterion: 19\n",
      "   Classification: 11\n",
      "   Threshold: 5\n",
      "   Exception: 3\n",
      "\n",
      "üîç Validando DAG...\n",
      "‚úÖ Grafo √© DAG v√°lido\n"
     ]
    }
   ],
   "source": [
    "def projetar_policy_graph(kg: nx.DiGraph) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Projeta KG em Policy Graph decis√≥rio.\n",
    "    \n",
    "    Args:\n",
    "        kg: Knowledge Graph completo\n",
    "        \n",
    "    Returns:\n",
    "        Policy Graph (DAG)\n",
    "    \"\"\"\n",
    "    policy = nx.DiGraph()\n",
    "    \n",
    "    # Tipos de n√≥s relevantes para decis√£o\n",
    "    decision_node_types = {\"IncidentType\", \"Criterion\", \"Threshold\", \"Exception\", \"Classification\"}\n",
    "    \n",
    "    # Tipos de rela√ß√µes relevantes\n",
    "    decision_edge_types = {\"DEPENDS_ON\", \"HAS_THRESHOLD\", \"HAS_EXCEPTION\", \"IMPLIES\", \"REQUIRES\", \"CLASSIFIED_AS\"}\n",
    "    \n",
    "    # Adicionar n√≥s decis√≥rios\n",
    "    for node_id, data in kg.nodes(data=True):\n",
    "        if data.get('type') in decision_node_types:\n",
    "            policy.add_node(node_id, **data)\n",
    "    \n",
    "    # Adicionar arestas decis√≥rias\n",
    "    for source, target, data in kg.edges(data=True):\n",
    "        if data.get('type') in decision_edge_types:\n",
    "            if source in policy and target in policy:\n",
    "                policy.add_edge(source, target, **data)\n",
    "    \n",
    "    # Transformar Thresholds em Criterions quando necess√°rio\n",
    "    threshold_nodes = [n for n in policy.nodes() if policy.nodes[n]['type'] == 'Threshold']\n",
    "    \n",
    "    for threshold_id in threshold_nodes:\n",
    "        # Criar criterion associado\n",
    "        threshold_data = policy.nodes[threshold_id]\n",
    "        criterion_id = threshold_id.replace('Threshold:', 'Criterion:')\n",
    "        \n",
    "        if criterion_id not in policy:\n",
    "            policy.add_node(criterion_id,\n",
    "                          type='Criterion',\n",
    "                          id=threshold_data['id'],\n",
    "                          properties={'derived_from_threshold': threshold_id})\n",
    "            \n",
    "            # Conectar criterion ao threshold\n",
    "            policy.add_edge(criterion_id, threshold_id, type='HAS_THRESHOLD')\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "def validar_e_corrigir_dag(graph: nx.DiGraph) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Valida se √© DAG e remove ciclos se necess√°rio.\n",
    "    \n",
    "    Args:\n",
    "        graph: Grafo a validar\n",
    "        \n",
    "    Returns:\n",
    "        DAG v√°lido\n",
    "    \"\"\"\n",
    "    if nx.is_directed_acyclic_graph(graph):\n",
    "        print(\"‚úÖ Grafo √© DAG v√°lido\")\n",
    "        return graph\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Grafo cont√©m ciclos. Removendo...\")\n",
    "    \n",
    "    # Encontrar ciclos\n",
    "    try:\n",
    "        cycles = list(nx.simple_cycles(graph))\n",
    "        print(f\"   Encontrados {len(cycles)} ciclos\")\n",
    "        \n",
    "        # Remover arestas IMPLIES de ciclos (menor impacto)\n",
    "        for cycle in cycles:\n",
    "            # Encontrar aresta IMPLIES no ciclo\n",
    "            for i in range(len(cycle)):\n",
    "                source = cycle[i]\n",
    "                target = cycle[(i + 1) % len(cycle)]\n",
    "                \n",
    "                if graph.has_edge(source, target):\n",
    "                    edge_type = graph[source][target].get('type')\n",
    "                    if edge_type == 'IMPLIES':\n",
    "                        graph.remove_edge(source, target)\n",
    "                        print(f\"   Removida aresta {source} -> {target}\")\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao processar ciclos: {e}\")\n",
    "    \n",
    "    # Verificar novamente\n",
    "    if nx.is_directed_acyclic_graph(graph):\n",
    "        print(\"‚úÖ DAG corrigido com sucesso\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Grafo ainda cont√©m ciclos ap√≥s corre√ß√£o\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "# Gerar Policy Graph\n",
    "print(\"\\nüéØ Gerando Policy Graph...\\n\")\n",
    "\n",
    "policy_graph = projetar_policy_graph(kg)\n",
    "\n",
    "print(f\"\\nüìä Policy Graph:\")\n",
    "print(f\"   N√≥s: {policy_graph.number_of_nodes()}\")\n",
    "print(f\"   Arestas: {policy_graph.number_of_edges()}\")\n",
    "\n",
    "# Estat√≠sticas por tipo\n",
    "policy_node_types = Counter(policy_graph.nodes[n]['type'] for n in policy_graph.nodes())\n",
    "print(f\"\\nüìä Distribui√ß√£o de N√≥s (Policy):\")\n",
    "for node_type, count in policy_node_types.most_common():\n",
    "    print(f\"   {node_type}: {count}\")\n",
    "\n",
    "# Validar DAG\n",
    "print(\"\\nüîç Validando DAG...\")\n",
    "policy_graph = validar_e_corrigir_dag(policy_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Salvando Policy Graph...\n",
      "   ‚úÖ JSON: ../artifacts/anp_policy.json\n"
     ]
    }
   ],
   "source": [
    "# Exportar Policy Graph\n",
    "print(\"\\nüíæ Salvando Policy Graph...\")\n",
    "\n",
    "# JSON\n",
    "policy_json = nx.node_link_data(policy_graph)\n",
    "with open(POLICY_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(policy_json, f, ensure_ascii=False, indent=2)\n",
    "print(f\"   ‚úÖ JSON: {POLICY_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] Compila√ß√£o Policy ‚Üí √Årvore com Subpolicies\n",
    "\n",
    "Compila Policy Graph em √°rvore JSON com ramifica√ß√£o por subpolicies detectadas automaticamente.\n",
    "\n",
    "**Estrat√©gia**:\n",
    "1. Criar n√≥ raiz roteador (\"Qual o tipo de ocorr√™ncia?\")\n",
    "2. Para cada subpolicy: extrair subgrafo e compilar sub√°rvore\n",
    "3. Anexar sub√°rvores como ramos da raiz\n",
    "4. Resultado: √°rvore com branching sem√¢ntico, menor entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Tuple\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "# üîπ Utilit√°rio: ordenar n√≥s decis√≥rios dentro da subpolicy\n",
    "\n",
    "def ordenar_nos_decisao(subgraph: nx.DiGraph) -> List[str]:\n",
    "    \"\"\"\n",
    "    Define ordem decis√≥ria dentro de uma subpolicy.\n",
    "    Prioriza preced√™ncia estrutural (topological sort).\n",
    "    \"\"\"\n",
    "    decision_nodes = [\n",
    "        n for n, d in subgraph.nodes(data=True)\n",
    "        if d.get(\"type\") in {\"Criterion\", \"Threshold\"}\n",
    "    ]\n",
    "\n",
    "    decision_subgraph = subgraph.subgraph(decision_nodes)\n",
    "\n",
    "    try:\n",
    "        return list(nx.topological_sort(decision_subgraph))\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        # Fallback: n√≥s mais conectados primeiro\n",
    "        return sorted(decision_nodes, key=lambda n: subgraph.degree(n), reverse=True)\n",
    "\n",
    "#üîπ Constru√ß√£o recursiva de subnodos (mantida, mas agora com fluxo)\n",
    "\n",
    "def construir_subnodos(\n",
    "    policy: nx.DiGraph,\n",
    "    node_id: str,\n",
    "    visited: set = None,\n",
    "    depth: int = 0,\n",
    "    max_depth: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Constr√≥i subnodos recursivamente com encerramento local.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if depth > max_depth or node_id in visited:\n",
    "        return []\n",
    "\n",
    "    visited.add(node_id)\n",
    "    subnodos = []\n",
    "\n",
    "    # 1Ô∏è‚É£ Encerramentos locais\n",
    "    for succ in policy.successors(node_id):\n",
    "        if policy.nodes[succ].get(\"type\") == \"Classification\":\n",
    "            subnodos.append({\n",
    "                \"id\": succ.replace(':', '_').replace(' ', '_').lower(),\n",
    "                \"tipo\": \"terminal\",\n",
    "                \"classe\": policy.nodes[succ].get(\"id\", \"Classe n√£o especificada\")\n",
    "            })\n",
    "\n",
    "    # 2Ô∏è‚É£ Continua√ß√£o do fluxo\n",
    "    for succ in policy.successors(node_id):\n",
    "        succ_type = policy.nodes[succ].get(\"type\")\n",
    "\n",
    "        if succ_type in {\"Criterion\", \"Threshold\"}:\n",
    "            children = construir_subnodos(\n",
    "                policy,\n",
    "                succ,\n",
    "                visited.copy(),\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "\n",
    "            if not children:\n",
    "                children = [{\n",
    "                    \"id\": f\"{succ}_default\".replace(':', '_').replace(' ', '_').lower(),\n",
    "                    \"tipo\": \"terminal\",\n",
    "                    \"classe\": \"Requer an√°lise adicional\"\n",
    "                }]\n",
    "\n",
    "            subnodos.append({\n",
    "                \"id\": succ.replace(':', '_').replace(' ', '_').lower(),\n",
    "                \"pergunta\": policy.nodes[succ].get(\"id\", \"Crit√©rio\"),\n",
    "                \"tipo\": \"decisao\",\n",
    "                \"subnodos\": children\n",
    "            })\n",
    "\n",
    "    return subnodos\n",
    "# üîπ Compila√ß√£o de uma subpolicy (CORRIGIDA)\n",
    "\n",
    "def compilar_subarvore(policy_subgraph: nx.DiGraph, subpolicy_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compila um subgrafo (subpolicy) em sub√°rvore com ordem decis√≥ria expl√≠cita.\n",
    "    \"\"\"\n",
    "\n",
    "    ordered_nodes = ordenar_nos_decisao(policy_subgraph)\n",
    "\n",
    "    if not ordered_nodes:\n",
    "        return {\n",
    "            \"id\": subpolicy_id,\n",
    "            \"tipo\": \"terminal\",\n",
    "            \"classe\": \"Subpolicy sem crit√©rios execut√°veis\"\n",
    "        }\n",
    "\n",
    "    root_node_id = ordered_nodes[0]\n",
    "    root_data = policy_subgraph.nodes[root_node_id]\n",
    "\n",
    "    subpolicy_root = {\n",
    "        \"id\": subpolicy_id,\n",
    "        \"pergunta\": root_data.get(\"id\", f\"Dom√≠nio normativo {subpolicy_id}\"),\n",
    "        \"tipo\": \"decisao\",\n",
    "        \"subnodos\": construir_subnodos(\n",
    "            policy_subgraph,\n",
    "            root_node_id,\n",
    "            visited=set(),\n",
    "            depth=0\n",
    "        )\n",
    "    }\n",
    "\n",
    "    if not subpolicy_root[\"subnodos\"]:\n",
    "        subpolicy_root[\"subnodos\"].append({\n",
    "            \"id\": f\"{subpolicy_id}_terminal\",\n",
    "            \"tipo\": \"terminal\",\n",
    "            \"classe\": \"Requer an√°lise t√©cnica espec√≠fica\"\n",
    "        })\n",
    "\n",
    "    return subpolicy_root\n",
    "#üîπ Compilador final com subpolicies (plug-and-play)\n",
    "\n",
    "def compilar_arvore_com_subpolicies(\n",
    "    policy: nx.DiGraph,\n",
    "    communities: List[set]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compila Policy Graph em √°rvore com ramifica√ß√£o autom√°tica por subpolicies.\n",
    "    \"\"\"\n",
    "\n",
    "    raiz = {\n",
    "        \"id\": \"raiz\",\n",
    "        \"pergunta\": \"Qual o tipo de ocorr√™ncia?\",\n",
    "        \"tipo\": \"decisao\",\n",
    "        \"subnodos\": []\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüå≥ Compilando √°rvore com {len(communities)} subpolicies...\\n\")\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        subpolicy_id = f\"subpolicy_{i}\"\n",
    "\n",
    "        nodes_in_subpolicy = set(community)\n",
    "\n",
    "        # incluir classifica√ß√µes associadas\n",
    "        for n in community:\n",
    "            for succ in policy.successors(n):\n",
    "                if policy.nodes[succ].get(\"type\") == \"Classification\":\n",
    "                    nodes_in_subpolicy.add(succ)\n",
    "\n",
    "        subgraph = policy.subgraph(nodes_in_subpolicy).copy()\n",
    "\n",
    "        print(f\"   Subpolicy {i}:\")\n",
    "        print(f\"      N√≥s: {subgraph.number_of_nodes()}\")\n",
    "        print(f\"      Arestas: {subgraph.number_of_edges()}\")\n",
    "\n",
    "        subarvore = compilar_subarvore(subgraph, subpolicy_id)\n",
    "        raiz[\"subnodos\"].append(subarvore)\n",
    "\n",
    "    if not raiz[\"subnodos\"]:\n",
    "        raiz[\"subnodos\"].append({\n",
    "            \"id\": \"incidente_generico\",\n",
    "            \"tipo\": \"terminal\",\n",
    "            \"classe\": \"Classifica√ß√£o n√£o determinada\"\n",
    "        })\n",
    "\n",
    "    return raiz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## [7.5] Detec√ß√£o Autom√°tica de Subpolicies\n\nUtiliza expans√£o controlada baseada em **√¢ncoras normativas** (IncidentType e Classification) para detectar subpolicies no Policy Graph.\n\n**Abordagem**:\n- **√Çncoras Normativas**: N√≥s do tipo IncidentType e Classification servem como pontos de partida\n- **Expans√£o Controlada**: Navega√ß√£o bidirecional (predecessores + sucessores) com profundidade limitada\n- **Evita Duplica√ß√£o**: Subpolicies com pouca novidade (< 5 n√≥s novos) s√£o descartadas\n\n**Objetivo**: Ramificar √°rvore por dom√≠nios normativos detectados automaticamente, reduzindo entropia."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Detectando subpolicies automaticamente...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#üîπ 1. Preparar grafo para detec√ß√£o de comunidades\n",
    "print(\"\\nüß† Detectando subpolicies automaticamente...\\n\")\n",
    "\n",
    "def detectar_ancoras_normativas(policy: nx.DiGraph):\n",
    "    anchors = []\n",
    "\n",
    "    for n, data in policy.nodes(data=True):\n",
    "        if data.get(\"type\") in [\"IncidentType\", \"Classification\"]:\n",
    "            anchors.append(n)\n",
    "\n",
    "    return anchors\n",
    "\n",
    "def expandir_subpolicy(policy: nx.DiGraph, anchor: str, max_depth=5):\n",
    "    visited = set()\n",
    "    queue = [(anchor, 0)]\n",
    "    subpolicy_nodes = set([anchor])\n",
    "\n",
    "    while queue:\n",
    "        node, depth = queue.pop(0)\n",
    "        if depth >= max_depth:\n",
    "            continue\n",
    "\n",
    "        for succ in policy.successors(node):\n",
    "            if succ not in visited:\n",
    "                visited.add(succ)\n",
    "                subpolicy_nodes.add(succ)\n",
    "                queue.append((succ, depth + 1))\n",
    "\n",
    "        for pred in policy.predecessors(node):\n",
    "            if pred not in visited:\n",
    "                visited.add(pred)\n",
    "                subpolicy_nodes.add(pred)\n",
    "                queue.append((pred, depth + 1))\n",
    "\n",
    "    return subpolicy_nodes\n",
    "\n",
    "anchors = detectar_ancoras_normativas(policy_graph)\n",
    "\n",
    "subpolicies = []\n",
    "used_nodes = set()\n",
    "\n",
    "for i, anchor in enumerate(anchors):\n",
    "    nodes = expandir_subpolicy(policy_graph, anchor)\n",
    "\n",
    "    # Evitar subpolicies duplicadas\n",
    "    if len(nodes - used_nodes) < 5:\n",
    "        continue\n",
    "\n",
    "    subpolicies.append(nodes)\n",
    "    used_nodes |= nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#üîπ Execu√ß√£o + estat√≠sticas\n\nprint(\"\\nüå≥ Compilando √°rvore de decis√£o com subpolicies...\\n\")\n\narvore_decisao = compilar_arvore_com_subpolicies(policy_graph, subpolicies)\n\nwith open(TREE_JSON, 'w', encoding='utf-8') as f:\n    json.dump(arvore_decisao, f, ensure_ascii=False, indent=2)\n\nprint(f\"\\n‚úÖ √Årvore de decis√£o salva em: {TREE_JSON}\")\n\ndef contar_nos_recursivo(node: Dict[str, Any]) -> Tuple[int, int]:\n    if node.get(\"tipo\") == \"terminal\":\n        return 0, 1\n\n    decisao, terminal = 1, 0\n    for sub in node.get(\"subnodos\", []):\n        d, t = contar_nos_recursivo(sub)\n        decisao += d\n        terminal += t\n    return decisao, terminal\n\n\nnum_decisao, num_terminal = contar_nos_recursivo(arvore_decisao)\n\nprint(f\"\\nüìä Estat√≠sticas da √Årvore (COM SUBPOLICIES):\")\nprint(f\"   N√≥s de decis√£o: {num_decisao}\")\nprint(f\"   N√≥s terminais: {num_terminal}\")\nprint(f\"   Total: {num_decisao + num_terminal}\")\nprint(f\"   Subpolicies (ramos principais): {len(subpolicies)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] Compila√ß√£o Policy ‚Üí √Årvore\n",
    "\n",
    "Compila Policy Graph em √°rvore JSON compat√≠vel com classificador modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≥ Compilando Policy Graph em √°rvore de decis√£o...\n",
      "\n",
      "‚úÖ √Årvore de decis√£o salva em: ../artifacts/anp_tree.json\n",
      "\n",
      "üìä Estat√≠sticas da √Årvore:\n",
      "   N√≥s de decis√£o: 2\n",
      "   N√≥s terminais: 1\n",
      "   Total: 3\n"
     ]
    }
   ],
   "source": [
    "def compilar_arvore_decisao(policy: nx.DiGraph) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compila Policy Graph em √°rvore de decis√£o JSON.\n",
    "    \n",
    "    Args:\n",
    "        policy: Policy Graph (DAG)\n",
    "        \n",
    "    Returns:\n",
    "        √Årvore de decis√£o em formato JSON\n",
    "    \"\"\"\n",
    "    # Encontrar roots (IncidentTypes sem predecessores)\n",
    "    roots = [n for n in policy.nodes() \n",
    "            if policy.nodes[n]['type'] == 'IncidentType' \n",
    "            and policy.in_degree(n) == 0]\n",
    "    \n",
    "    if not roots:\n",
    "        # Fallback: pegar todos IncidentTypes\n",
    "        roots = [n for n in policy.nodes() if policy.nodes[n]['type'] == 'IncidentType']\n",
    "    \n",
    "    # Criar n√≥ raiz \"Qual o tipo de ocorr√™ncia?\"\n",
    "    raiz = {\n",
    "        \"id\": \"raiz\",\n",
    "        \"pergunta\": \"Qual o tipo de ocorr√™ncia?\",\n",
    "        \"tipo\": \"decisao\",\n",
    "        \"subnodos\": []\n",
    "    }\n",
    "    \n",
    "    # Para cada IncidentType, criar sub√°rvore\n",
    "    for root_id in roots:\n",
    "        root_data = policy.nodes[root_id]\n",
    "        \n",
    "        # Criar n√≥ para este tipo de incidente\n",
    "        incident_node = {\n",
    "            \"id\": root_id.replace(':', '_').replace(' ', '_').lower(),\n",
    "            \"pergunta\": root_data.get('id', 'Tipo de incidente'),\n",
    "            \"tipo\": \"decisao\",\n",
    "            \"subnodos\": []\n",
    "        }\n",
    "        \n",
    "        # Construir sub√°rvore a partir deste n√≥\n",
    "        subnodos = construir_subnodos(policy, root_id)\n",
    "        incident_node[\"subnodos\"] = subnodos\n",
    "        \n",
    "        raiz[\"subnodos\"].append(incident_node)\n",
    "    \n",
    "    # Se n√£o houver roots, criar estrutura m√≠nima\n",
    "    if not raiz[\"subnodos\"]:\n",
    "        raiz[\"subnodos\"].append({\n",
    "            \"id\": \"incidente_generico\",\n",
    "            \"pergunta\": \"Incidente gen√©rico\",\n",
    "            \"tipo\": \"decisao\",\n",
    "            \"subnodos\": [\n",
    "                {\n",
    "                    \"id\": \"classe_desconhecida\",\n",
    "                    \"tipo\": \"terminal\",\n",
    "                    \"classe\": \"Classifica√ß√£o n√£o determinada\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    return raiz\n",
    "\n",
    "\n",
    "def construir_subnodos(policy: nx.DiGraph, node_id: str, visited: set = None, depth: int = 0) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Constr√≥i subnodos recursivamente a partir de um n√≥.\n",
    "    \n",
    "    Args:\n",
    "        policy: Policy Graph\n",
    "        node_id: ID do n√≥ atual\n",
    "        visited: Conjunto de n√≥s j√° visitados (evitar ciclos)\n",
    "        depth: Profundidade atual (limitar recurs√£o)\n",
    "        \n",
    "    Returns:\n",
    "        Lista de subnodos\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    # Limitar profundidade para evitar recurs√£o infinita\n",
    "    if depth > 10 or node_id in visited:\n",
    "        return []\n",
    "    \n",
    "    visited.add(node_id)\n",
    "    subnodos = []\n",
    "    \n",
    "    # Obter sucessores (ordenados por tipo de rela√ß√£o)\n",
    "    successors = list(policy.successors(node_id))\n",
    "    \n",
    "    for succ_id in successors:\n",
    "        succ_data = policy.nodes[succ_id]\n",
    "        edge_data = policy[node_id][succ_id]\n",
    "        \n",
    "        # Se sucessor √© Classification, criar n√≥ terminal\n",
    "        if succ_data['type'] == 'Classification':\n",
    "            subnodos.append({\n",
    "                \"id\": succ_id.replace(':', '_').replace(' ', '_').lower(),\n",
    "                \"tipo\": \"terminal\",\n",
    "                \"classe\": succ_data.get('id', 'Classe n√£o especificada')\n",
    "            })\n",
    "        \n",
    "        # Se sucessor √© Criterion ou Threshold, criar n√≥ de decis√£o\n",
    "        elif succ_data['type'] in ['Criterion', 'Threshold']:\n",
    "            # Construir pergunta\n",
    "            pergunta = succ_data.get('id', 'Crit√©rio')\n",
    "            \n",
    "            # Recursivamente construir subnodos\n",
    "            sub_subnodos = construir_subnodos(policy, succ_id, visited.copy(), depth + 1)\n",
    "            \n",
    "            # Se n√£o houver subnodos, criar terminal padr√£o\n",
    "            if not sub_subnodos:\n",
    "                sub_subnodos = [{\n",
    "                    \"id\": f\"{succ_id}_default\".replace(':', '_').replace(' ', '_').lower(),\n",
    "                    \"tipo\": \"terminal\",\n",
    "                    \"classe\": \"Requer an√°lise adicional\"\n",
    "                }]\n",
    "            \n",
    "            subnodos.append({\n",
    "                \"id\": succ_id.replace(':', '_').replace(' ', '_').lower(),\n",
    "                \"pergunta\": pergunta,\n",
    "                \"tipo\": \"decisao\",\n",
    "                \"subnodos\": sub_subnodos\n",
    "            })\n",
    "    \n",
    "    return subnodos\n",
    "\n",
    "\n",
    "# Compilar √°rvore\n",
    "print(\"\\nüå≥ Compilando Policy Graph em √°rvore de decis√£o...\\n\")\n",
    "\n",
    "arvore_decisao = compilar_arvore_decisao(policy_graph)\n",
    "\n",
    "# Salvar √°rvore\n",
    "with open(TREE_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(arvore_decisao, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ √Årvore de decis√£o salva em: {TREE_JSON}\")\n",
    "\n",
    "# Estat√≠sticas da √°rvore\n",
    "def contar_nos_recursivo(node: Dict[str, Any]) -> Tuple[int, int]:\n",
    "    \"\"\"Conta n√≥s de decis√£o e terminais recursivamente.\"\"\"\n",
    "    if node.get('tipo') == 'terminal':\n",
    "        return 0, 1\n",
    "    \n",
    "    decisao = 1\n",
    "    terminal = 0\n",
    "    \n",
    "    for subnode in node.get('subnodos', []):\n",
    "        d, t = contar_nos_recursivo(subnode)\n",
    "        decisao += d\n",
    "        terminal += t\n",
    "    \n",
    "    return decisao, terminal\n",
    "\n",
    "num_decisao, num_terminal = contar_nos_recursivo(arvore_decisao)\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas da √Årvore:\")\n",
    "print(f\"   N√≥s de decis√£o: {num_decisao}\")\n",
    "print(f\"   N√≥s terminais: {num_terminal}\")\n",
    "print(f\"   Total: {num_decisao + num_terminal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [9] Relat√≥rio de Qualidade\n",
    "\n",
    "Gera estat√≠sticas e visualiza√ß√µes sobre os artefatos criados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"üìä RELAT√ìRIO DE QUALIDADE\")\nprint(\"=\"*80)\n\n# 1. Corpus\nprint(\"\\n1Ô∏è‚É£ CORPUS\")\nprint(f\"   Documentos processados: {len(clean_docs)}\")\nprint(f\"   Chunks gerados: {len(all_chunks)}\")\nprint(f\"   M√©dia chunks/doc: {len(all_chunks) / len(clean_docs):.1f}\")\n\n# 2. Knowledge Graph\nprint(\"\\n2Ô∏è‚É£ KNOWLEDGE GRAPH\")\nprint(f\"   Total de n√≥s: {kg.number_of_nodes()}\")\nprint(f\"   Total de arestas: {kg.number_of_edges()}\")\nprint(f\"   Densidade: {nx.density(kg):.4f}\")\n\nprint(\"\\n   Distribui√ß√£o de n√≥s por tipo:\")\nfor node_type, count in node_types.most_common():\n    print(f\"      {node_type}: {count}\")\n\nedge_types = Counter(kg[u][v]['type'] for u, v in kg.edges())\nprint(\"\\n   Distribui√ß√£o de arestas por tipo:\")\nfor edge_type, count in edge_types.most_common():\n    print(f\"      {edge_type}: {count}\")\n\n# 3. Policy Graph\nprint(\"\\n3Ô∏è‚É£ POLICY GRAPH\")\nprint(f\"   Total de n√≥s: {policy_graph.number_of_nodes()}\")\nprint(f\"   Total de arestas: {policy_graph.number_of_edges()}\")\nprint(f\"   √â DAG: {'‚úÖ' if nx.is_directed_acyclic_graph(policy_graph) else '‚ùå'}\")\n\nif policy_graph.number_of_nodes() > 0:\n    print(\"\\n   Distribui√ß√£o de n√≥s por tipo:\")\n    for node_type, count in policy_node_types.most_common():\n        print(f\"      {node_type}: {count}\")\n\n# 3.5. Detec√ß√£o de Subpolicies (Normativa-Sem√¢ntica)\nprint(\"\\n3Ô∏è‚É£.5Ô∏è‚É£ DETEC√á√ÉO DE SUBPOLICIES (√ÇNCORAS NORMATIVAS)\")\nprint(f\"   Subpolicies detectadas: {len(subpolicies)}\")\nprint(f\"   √Çncoras encontradas: {len(anchors)}\")\n\nprint(\"\\n   Distribui√ß√£o de tamanho das subpolicies:\")\nfor i, sp in enumerate(subpolicies):\n    print(f\"      subpolicy_{i}: {len(sp)} n√≥s\")\n\n# 4. √Årvore de Decis√£o\nprint(\"\\n4Ô∏è‚É£ √ÅRVORE DE DECIS√ÉO (COM SUBPOLICIES)\")\nprint(f\"   N√≥s de decis√£o: {num_decisao}\")\nprint(f\"   N√≥s terminais (classes): {num_terminal}\")\nprint(f\"   Subpolicies (ramos principais): {len(subpolicies)}\")\n\n# Profundidade m√©dia\ndef calcular_profundidade_media(node: Dict[str, Any], depth: int = 0) -> List[int]:\n    \"\"\"Calcula profundidades de todos os n√≥s terminais.\"\"\"\n    if node.get('tipo') == 'terminal':\n        return [depth]\n    \n    depths = []\n    for subnode in node.get('subnodos', []):\n        depths.extend(calcular_profundidade_media(subnode, depth + 1))\n    \n    return depths\n\ndepths = calcular_profundidade_media(arvore_decisao)\nif depths:\n    print(f\"   Profundidade m√©dia: {sum(depths) / len(depths):.1f}\")\n    print(f\"   Profundidade m√≠nima: {min(depths)}\")\n    print(f\"   Profundidade m√°xima: {max(depths)}\")\n\n# Fator de ramifica√ß√£o (branching factor) m√©dio\ndef calcular_branching_factor(node: Dict[str, Any]) -> List[int]:\n    \"\"\"Calcula branching factor de todos os n√≥s de decis√£o.\"\"\"\n    if node.get('tipo') == 'terminal':\n        return []\n    \n    factors = [len(node.get('subnodos', []))]\n    \n    for subnode in node.get('subnodos', []):\n        factors.extend(calcular_branching_factor(subnode))\n    \n    return factors\n\nbranching_factors = calcular_branching_factor(arvore_decisao)\nif branching_factors:\n    print(f\"   Branching factor m√©dio: {sum(branching_factors) / len(branching_factors):.1f}\")\n    print(f\"   Branching factor m√°ximo: {max(branching_factors)}\")\n\n# 5. Crit√©rios mais centrais (por degree no Policy Graph)\nif policy_graph.number_of_nodes() > 0:\n    print(\"\\n5Ô∏è‚É£ TOP 20 CRIT√âRIOS MAIS CENTRAIS (por degree)\")\n    \n    criterion_nodes_list = [n for n in policy_graph.nodes() if policy_graph.nodes[n]['type'] == 'Criterion']\n    \n    if criterion_nodes_list:\n        degrees = [(n, policy_graph.degree(n)) for n in criterion_nodes_list]\n        degrees.sort(key=lambda x: x[1], reverse=True)\n        \n        for i, (node_id, degree) in enumerate(degrees[:20], 1):\n            node_name = policy_graph.nodes[node_id].get('id', node_id)\n            print(f\"   {i:2d}. {node_name[:50]:<50} (degree: {degree})\")\n    else:\n        print(\"   Nenhum crit√©rio encontrado\")\n\n# 6. Exemplos de trilhas (paths)\nprint(\"\\n6Ô∏è‚É£ EXEMPLOS DE TRILHAS (10 amostras aleat√≥rias da raiz at√© folha)\")\n\ndef gerar_trilhas_aleatorias(node: Dict[str, Any], path: List[str] = None, max_trilhas: int = 10) -> List[List[str]]:\n    \"\"\"Gera trilhas aleat√≥rias da raiz at√© folhas.\"\"\"\n    if path is None:\n        path = []\n    \n    path = path + [node.get('pergunta', node.get('classe', node.get('id', 'N/A'))[:50])]\n    \n    if node.get('tipo') == 'terminal':\n        return [path]\n    \n    all_paths = []\n    for subnode in node.get('subnodos', []):\n        all_paths.extend(gerar_trilhas_aleatorias(subnode, path, max_trilhas))\n        if len(all_paths) >= max_trilhas:\n            break\n    \n    return all_paths[:max_trilhas]\n\ntrilhas = gerar_trilhas_aleatorias(arvore_decisao, max_trilhas=10)\n\nfor i, trilha in enumerate(trilhas, 1):\n    print(f\"\\n   Trilha {i}:\")\n    for j, step in enumerate(trilha):\n        indent = \"   \" * (j + 1)\n        print(f\"{indent}{'‚îî‚îÄ' if j == len(trilha) - 1 else '‚îú‚îÄ'} {step}\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] Smoke Test Local\n",
    "\n",
    "Valida a √°rvore JSON com um evento fict√≠cio de exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ SMOKE TEST - Valida√ß√£o da √Årvore\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evento de exemplo\n",
    "evento_exemplo = \"\"\"\n",
    "Vazamento de 15m¬≥ de √≥leo diesel durante opera√ß√£o de abastecimento de embarca√ß√£o.\n",
    "Houve contamina√ß√£o de solo e pequeno impacto em corpo h√≠drico pr√≥ximo.\n",
    "Nenhum trabalhador ferido. Opera√ß√£o de conten√ß√£o realizada em 4 horas.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Evento de Exemplo:\")\n",
    "print(evento_exemplo)\n",
    "\n",
    "print(\"\\nüîç Navega√ß√£o Manual pela √Årvore:\\n\")\n",
    "\n",
    "# Fun√ß√£o auxiliar para navega√ß√£o\n",
    "def navegar_arvore_manual(node: Dict[str, Any], level: int = 0):\n",
    "    \"\"\"Exibe estrutura da √°rvore para navega√ß√£o manual.\"\"\"\n",
    "    indent = \"  \" * level\n",
    "    \n",
    "    if node.get('tipo') == 'terminal':\n",
    "        print(f\"{indent}üèÅ TERMINAL: {node.get('classe', 'N/A')}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{indent}‚ùì {node.get('pergunta', 'N/A')}\")\n",
    "    \n",
    "    # Mostrar primeiras 3 op√ß√µes\n",
    "    for i, subnode in enumerate(node.get('subnodos', [])[:3], 1):\n",
    "        print(f\"{indent}   {i}. Op√ß√£o: {subnode.get('pergunta', subnode.get('classe', subnode.get('id', 'N/A')))}\")\n",
    "    \n",
    "    if len(node.get('subnodos', [])) > 3:\n",
    "        print(f\"{indent}   ... (mais {len(node['subnodos']) - 3} op√ß√µes)\")\n",
    "\n",
    "# Exibir estrutura da raiz\n",
    "print(\"RAIZ:\")\n",
    "navegar_arvore_manual(arvore_decisao)\n",
    "\n",
    "# Simular sele√ß√£o de caminho baseado no evento\n",
    "print(\"\\nüéØ Caminho Simulado (baseado no evento):\")\n",
    "print(\"\\n1. Pergunta: 'Qual o tipo de ocorr√™ncia?'\")\n",
    "print(\"   Resposta: 'Acidente com Impacto no Meio Ambiente' (baseado em 'vazamento', 'contamina√ß√£o')\")\n",
    "\n",
    "# Se houver subnodos, navegar pelo primeiro\n",
    "if arvore_decisao.get('subnodos'):\n",
    "    # Tentar encontrar n√≥ relacionado a meio ambiente\n",
    "    meio_ambiente_node = None\n",
    "    for subnode in arvore_decisao['subnodos']:\n",
    "        pergunta = subnode.get('pergunta', '').lower()\n",
    "        if 'ambiente' in pergunta or 'meio' in pergunta:\n",
    "            meio_ambiente_node = subnode\n",
    "            break\n",
    "    \n",
    "    if not meio_ambiente_node:\n",
    "        meio_ambiente_node = arvore_decisao['subnodos'][0]\n",
    "    \n",
    "    print(f\"\\n2. N√≥ selecionado: {meio_ambiente_node.get('pergunta', meio_ambiente_node.get('id'))}\")\n",
    "    \n",
    "    if meio_ambiente_node.get('subnodos'):\n",
    "        print(f\"   Pr√≥ximas perguntas dispon√≠veis:\")\n",
    "        for i, sub in enumerate(meio_ambiente_node['subnodos'][:3], 1):\n",
    "            print(f\"      {i}. {sub.get('pergunta', sub.get('classe', 'N/A'))}\")\n",
    "\n",
    "print(\"\\n‚úÖ Valida√ß√£o da estrutura JSON:\")\n",
    "print(\"   - Raiz possui campo 'id': ‚úÖ\" if 'id' in arvore_decisao else \"   - Raiz FALTA campo 'id': ‚ùå\")\n",
    "print(\"   - Raiz possui campo 'pergunta': ‚úÖ\" if 'pergunta' in arvore_decisao else \"   - Raiz FALTA 'pergunta': ‚ùå\")\n",
    "print(\"   - Raiz possui campo 'tipo': ‚úÖ\" if 'tipo' in arvore_decisao else \"   - Raiz FALTA 'tipo': ‚ùå\")\n",
    "print(\"   - Raiz possui 'subnodos': ‚úÖ\" if 'subnodos' in arvore_decisao else \"   - Raiz FALTA 'subnodos': ‚ùå\")\n",
    "\n",
    "# Validar estrutura recursivamente\n",
    "def validar_estrutura(node: Dict[str, Any], path: str = \"raiz\") -> List[str]:\n",
    "    \"\"\"Valida estrutura da √°rvore recursivamente.\"\"\"\n",
    "    erros = []\n",
    "    \n",
    "    if 'id' not in node:\n",
    "        erros.append(f\"{path}: Falta campo 'id'\")\n",
    "    \n",
    "    if 'tipo' not in node:\n",
    "        erros.append(f\"{path}: Falta campo 'tipo'\")\n",
    "    elif node['tipo'] == 'terminal':\n",
    "        if 'classe' not in node:\n",
    "            erros.append(f\"{path}: N√≥ terminal sem campo 'classe'\")\n",
    "    elif node['tipo'] == 'decisao':\n",
    "        if 'pergunta' not in node:\n",
    "            erros.append(f\"{path}: N√≥ de decis√£o sem campo 'pergunta'\")\n",
    "        if 'subnodos' not in node:\n",
    "            erros.append(f\"{path}: N√≥ de decis√£o sem campo 'subnodos'\")\n",
    "        else:\n",
    "            for i, subnode in enumerate(node['subnodos']):\n",
    "                erros.extend(validar_estrutura(subnode, f\"{path}/subnodo[{i}]\"))\n",
    "    \n",
    "    return erros\n",
    "\n",
    "erros_estrutura = validar_estrutura(arvore_decisao)\n",
    "\n",
    "if erros_estrutura:\n",
    "    print(\"\\n‚ö†Ô∏è Erros de estrutura encontrados:\")\n",
    "    for erro in erros_estrutura[:10]:  # Mostrar primeiros 10\n",
    "        print(f\"   - {erro}\")\n",
    "    if len(erros_estrutura) > 10:\n",
    "        print(f\"   ... (mais {len(erros_estrutura) - 10} erros)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Estrutura da √°rvore v√°lida!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Resumo dos Artefatos Gerados\n",
    "\n",
    "Todos os artefatos foram salvos em `artifacts/`:\n",
    "\n",
    "1. **`anp_text_corpus.jsonl`** - Corpus de textos limpos (um documento por linha)\n",
    "2. **`anp_kg.graphml`** - Knowledge Graph completo (formato GraphML)\n",
    "3. **`anp_kg.json`** - Knowledge Graph completo (formato JSON)\n",
    "4. **`anp_policy.graphml`** - Policy Graph decis√≥rio (formato GraphML)\n",
    "5. **`anp_policy.json`** - Policy Graph decis√≥rio (formato JSON)\n",
    "6. **`anp_tree.json`** - √Årvore de decis√£o final (compat√≠vel com classificador)\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "\n",
    "1. **Valida√ß√£o manual**: Revisar `anp_tree.json` para garantir coer√™ncia normativa\n",
    "2. **Refinamento do KG**: Ajustar prompt guia e reprocessar chunks com melhor qualidade\n",
    "3. **Integra√ß√£o com LATS**: Carregar `anp_tree.json` no classificador LATS-P\n",
    "4. **Expans√£o de classes**: Adicionar mais informa√ß√µes normativas aos n√≥s terminais\n",
    "5. **Teste com eventos reais**: Validar √°rvore com casos de uso da ANP\n",
    "\n",
    "### Notas Importantes\n",
    "\n",
    "- ‚ö†Ô∏è **MODO DE TESTE ATIVO**: Apenas 10 chunks foram processados. Para produ√ß√£o, altere `TEST_MODE = False`\n",
    "- üí∞ **Custo de API**: Processar corpus completo pode custar significativamente em tokens Azure OpenAI\n",
    "- üîÑ **Reexecu√ß√£o**: Notebook √© idempotente - pode ser reexecutado para regenerar artefatos\n",
    "- üìä **Qualidade**: Qualidade final depende da qualidade dos PDFs e do prompt guia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir localiza√ß√£o dos artefatos\n",
    "print(\"\\nüì¶ ARTEFATOS GERADOS:\\n\")\n",
    "print(f\"   üìÑ Corpus:        {CORPUS_FILE}\")\n",
    "print(f\"   üï∏Ô∏è  KG (GraphML):  {KG_GRAPHML}\")\n",
    "print(f\"   üï∏Ô∏è  KG (JSON):     {KG_JSON}\")\n",
    "print(f\"   üéØ Policy (GraphML): {POLICY_GRAPHML}\")\n",
    "print(f\"   üéØ Policy (JSON):    {POLICY_JSON}\")\n",
    "print(f\"   üå≥ √Årvore (JSON):    {TREE_JSON}\")\n",
    "print(\"\\n‚úÖ Pipeline completo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}